{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78eee1a3-e0ba-4a23-9a3b-f205ebb1cdda",
   "metadata": {},
   "source": [
    "# **DATA SCIENCE CYCLE PHASES:**\n",
    "## The typical Data Science Cycle is divided into the following phases:\n",
    "* ### PHASE 1: Problem Definition\n",
    "* ### PHASE 2: Data Collection\n",
    "* ### PHASE 3: Data Preparation\n",
    "* ### PHASE 4: Data Exploration and Visualization\n",
    "* ### PHASE 5: Feature Engineering and Preprocessing\n",
    "* ### PHASE 6: Modeling\n",
    "* ### PHASE 7: Model Evaluation\n",
    "* ### PHASE 8: Deployment\n",
    "* ### PHASE 9: Monitoring and Maintenance\n",
    "* ### PHASE 10: Results Communication\n",
    "\n",
    " #########################################################################################################################\n",
    "\n",
    "## PHASE 1: Problem Definition\n",
    "\n",
    "**Description:** In this phase, the data scientist works closely with the stakeholders to understand the business problem and define the research question.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Stakeholder requirements: This input consists of the business objectives, scope, constraints, and other requirements provided by the stakeholders.\n",
    "* Domain knowledge: This input includes the knowledge of the industry, market trends, and other domain-specific information relevant to the problem.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Research question: The output of this phase is a clear and specific research question that defines the problem statement, the data that will be collected, and the outcome that will be achieved.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 2: Data Collection\n",
    "\n",
    "**Description:** In this phase, the data scientist collects the data required to answer the research question.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Research question: This input defines the data that needs to be collected to answer the research question.\n",
    "* Data sources: This input includes the sources from which the data will be collected, such as databases, APIs, files, or surveys.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Raw data: The output of this phase is the raw data collected from the data sources.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 3: Data Preparation\n",
    "\n",
    "**Description:** In this phase, the data scientist prepares the raw data for further analysis and modeling.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Raw data: This input is the data collected in the previous phase.\n",
    "* Data cleaning requirements: This input consists of the specific requirements for data cleaning, such as missing value imputation, outlier detection and removal, and data normalization.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Cleaned data: The output of this phase is the cleaned data, ready for exploration and modeling.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 4: Data Exploration and Visualization\n",
    "\n",
    "**Description:** In this phase, the data scientist explores the cleaned data to gain a deeper understanding of the relationships between the variables and the characteristics of the data.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Cleaned data: This input is the data prepared in the previous phase.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Insights and visualizations: The outputs of this phase are the insights gained from exploring the data and the visualizations that illustrate the patterns and relationships in the data.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 5: Feature Engineering and Preprocessing\n",
    "\n",
    "**Description:** In this phase, the data scientist creates new features from the existing data or preprocesses the data to make it suitable for modeling.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Cleaned data: This input is the data prepared in the previous phase.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Engineered or preprocessed data: The output of this phase is the data that has been transformed or engineered to make it suitable for modeling.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 6: Modeling\n",
    "\n",
    "**Description:** In this phase, the data scientist selects a suitable model, trains it on the data, and tests its performance.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Engineered or preprocessed data: This input is the data prepared in the previous phase.\n",
    "* Model selection criteria: This input consists of the criteria used to select a suitable model, such as accuracy, interpretability, and scalability.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Trained model: The output of this phase is the trained model that can be used to make predictions or generate insights.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 7: Model Evaluation\n",
    "\n",
    "**Description:** In this phase, the data scientist evaluates the performance of the model and fine-tunes its parameters to improve its accuracy and robustness.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Trained model: This input is the model trained in the previous phase.\n",
    "    Evaluation criteria: This input consists of the criteria used to evaluate the performance of the model. Classification models use metrics such as: accuracy, precision, recall, F1 score and ROC-AUC. Regression models use metrics such as: mean squared error, mean absolute error, negative mean absolute error, negative mean absolute error, mean absolute percentile error, r2, explained variance score, etc.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Improved model: The output of this phase is the model that has been fine-tuned to achieve better performance.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 8: Deployment\n",
    "\n",
    "**Description:** In this phase, the data scientist deploys the trained model into the production environment to make predictions or generate insights.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Improved model: This input is the model that has been fine-tuned to achieve better performance.\n",
    "* Production environment: This input consists of the hardware and software infrastructure used to deploy and run the model in production.\n",
    "* Deployment requirements: This input consists of the specific requirements for deploying the model, such as scalability, reliability, and security.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Deployed model: The output of this phase is the model that has been deployed in the production environment and is ready to be used for prediction or generating insights.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 9: Monitoring and Maintenance\n",
    "\n",
    "**Description:** In this phase, the data scientist monitors the performance of the deployed model and performs regular maintenance to ensure that it continues to perform well.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Deployed model: This input is the model that has been deployed in the production environment.\n",
    "* Performance metrics: This input consists of the metrics used to monitor the performance of the model. Classification models use metrics such as: accuracy, precision, recall, F1 score and ROC-AUC. Regression models use metrics such as: mean squared error, mean absolute error, negative mean absolute error, negative mean absolute error, mean absolute percentile error, r2, explained variance score, etc.\n",
    "* Maintenance requirements: This input consists of the specific requirements for maintaining the model, such as updating the model parameters or retraining the model with new data.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Maintained model: The output of this phase is the model that has been regularly maintained to ensure that it continues to perform well in the production environment.\n",
    "\n",
    " #########################################################################################################################\n",
    "## PHASE 10: Results Communication\n",
    "\n",
    "**Description:** This final phase focuses on presenting the insights gained from the data science project, which are communicated to stakeholders in a clear and understandable manner, and the results are shared in a way that is easily understood by non-technical audiences, such as executives or clients. \n",
    "\n",
    "It involves creating visualizations, reports, and presentations that effectively convey the results of the project and any insights that were gained from the data. The goal is to ensure that the stakeholders can understand the impact and implications of the project and make informed decisions based on the results.\n",
    "\n",
    "This phase also involves identifying areas for future improvement and considering the implications of the results for the organization as a whole.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* Maintained model: This input is the model that has been regularly maintained to ensure that it continues to perform well in the production environment.\n",
    "* Insights and results: This input consists of the insights and results generated by the model, such as predictions, recommendations, or visualizations.\n",
    "* Storytelling requirements: This input consists of the specific requirements for communicating the insights and results, such as audience, format, and tone.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* Data insights and story: The output of this phase is the communication of the insights and results generated by the model to the stakeholders, in a way that is clear, concise, and engaging. The data scientist tells the story of how the research question was answered, and how the insights and results can be used to inform decision-making or drive business value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9e555-3b14-45a5-a25c-b2944267f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'normalize': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Elastic Net Regression': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1, 10],\n",
    "            'l1_ratio': [0.25, 0.5, 0.75]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Regression': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [0.1, 1, 10],\n",
    "            'epsilon': [0.01, 0.1, 1]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting Regression': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.5, 0.75, 1]\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost Regression': {\n",
    "        'model': AdaBoostRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'loss': ['linear', 'square', 'exponential']\n",
    "        }\n",
    "    },\n",
    "    'XGBoost Regression': {\n",
    "        'model': XGBRegressor(),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'n_estimators': [50, 100, 150]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest Regression': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees Regression': {\n",
    "        'model': ExtraTreesRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdf3ad-2752-4818-a23c-a37ae5c7788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# assume X_train, y_train, X_valid, y_valid, X_test, y_test are pandas dataframes\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    # add your preprocessing steps here\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_valid_transformed = preprocessing_pipeline.transform(X_valid)\n",
    "X_test_transformed = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d3d25-57d3-4178-afe5-2c00c596ae8d",
   "metadata": {},
   "source": [
    "## Define models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12685d5-c181-42b4-b517-85bda770ad44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Linear Regression\u001b[39;00m\n\u001b[1;32m      4\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mLinearRegression\u001b[49m(),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Lasso Regression\u001b[39;00m\n\u001b[1;32m     12\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLasso Regression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: Lasso(),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     }\n\u001b[1;32m     19\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "# Linear Regression\n",
    "models['Linear Regression'] = {\n",
    "    'model': LinearRegression(),\n",
    "    'params': {\n",
    "        'normalize': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lasso Regression\n",
    "models['Lasso Regression'] = {\n",
    "    'model': Lasso(),\n",
    "    'params': {\n",
    "        'alpha': [0.1, 0.5, 1, 2, 5, 10],\n",
    "        'normalize': [True, False],\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ridge Regression\n",
    "models['Ridge Regression'] = {\n",
    "    'model': Ridge(),\n",
    "    'params': {\n",
    "        'alpha': [0.1, 0.5, 1, 2, 5, 10],\n",
    "        'normalize': [True, False],\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Support Vector Regression\n",
    "models['SVR'] = {\n",
    "    'model': SVR(),\n",
    "    'params': {\n",
    "        'C': [0.1, 0.5, 1, 2, 5, 10],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Random Forest Regression\n",
    "models['Random Forest Regression'] = {\n",
    "    'model': RandomForestRegressor(),\n",
    "    'params': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'min_samples_split': [2, 3, 4],\n",
    "        'min_samples_leaf': [1, 2, 3],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# XGBoost Regression\n",
    "models['XGBoost Regression'] = {\n",
    "    'model': XGBRegressor(),\n",
    "    'params': {\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019a029-1638-47bc-bd7f-9237a615eeea",
   "metadata": {},
   "source": [
    "## Let's first try a models dictionary with 1 model and only 4 possible combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c22b3b-947d-4911-b85d-f4aa6a57ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "# XGBoost Regression\n",
    "models['XGBoost Regression'] = {\n",
    "    'model': XGBRegressor(),\n",
    "    'params': {\n",
    "        'learning_rate': [0.05],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'n_estimators': [200],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fdf81-4e62-4187-be09-cfbc38431fc6",
   "metadata": {},
   "source": [
    "## Loop each model to train, tune, and retrain final model with best parameters and joined training+validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c53db-e11b-4293-ac60-46eed2db3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    gs = GridSearchCV(model['model'], model['params'], cv=5,\n",
    "                      scoring='neg_mean_squared_error', return_train_score=True,\n",
    "                      n_jobs=-1, verbose=1, refit=True)\n",
    "    gs.fit(X_train_transformed, y_train,\n",
    "           eval_set=[(X_valid_transformed, y_valid)],\n",
    "           early_stopping_rounds=10)\n",
    "\n",
    "    # Get the best estimator and store it\n",
    "    best_model = gs.best_estimator_\n",
    "    model['best_model'] = best_model\n",
    "\n",
    "    # Join training and validation datasets for retraining best model with best parameters\n",
    "    X_train_valid = pd.concat([X_train, X_valid], axis=0)\n",
    "    y_train_valid = pd.concat([y_train, y_valid], axis=0)\n",
    "    \n",
    "    # Retrain best model with joined training+validation dataset using best hyperparameters\n",
    "    best_model.fit(X_train_valid, y_train_valid)\n",
    "    \n",
    "    # Store the final best model retrained with joined training+validation dataset and best hyperparameters\n",
    "    model['final_best_model'] = best_model\n",
    "    \n",
    "    # Evaluate the model on the test set and store the predictions and metric score\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    model['test_predictions'] = y_test_pred\n",
    "    model['test_mse'] = test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6644d-bff9-492b-94b1-d3074a1afc19",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1a437-1d92-4ded-a6c8-20c7ac8de2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test mean squared error for each model (not ordered)\n",
    "for model_name, model in models.items():\n",
    "    print(f'{model_name} test mean squared error: {model[\"test_mse\"]:.2f}')\n",
    "\n",
    "# Print the top models with the lowest test mean squared error (in ascending order)\n",
    "sorted_models = sorted(models.items(), key=lambda x: x[1]['test_mse'])\n",
    "print('\\nTop Models:')\n",
    "for i in range(len(models)):\n",
    "    model_name, model = sorted_models[i]\n",
    "    print(f'{i+1}: {model_name} test mean squared error: {model[\"test_mse\"]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
